{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reasoning Approaches Experimentation Notebook\n",
        "\n",
        "This notebook allows you to experiment with different reasoning approaches across various LLM providers and models.\n",
        "\n",
        "## Features\n",
        "- Support for multiple providers: OpenRouter, Anthropic, Cohere\n",
        "- 10 different reasoning approaches\n",
        "- HuggingFace dataset integration\n",
        "- CSV export and HuggingFace Hub upload\n",
        "- Comprehensive experiment tracking\n",
        "\n",
        "## Setup Instructions\n",
        "1. Install required packages\n",
        "2. Set up API keys\n",
        "3. Load your dataset\n",
        "4. Configure experiments\n",
        "5. Run experiments\n",
        "6. Analyze results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q openai anthropic cohere datasets huggingface-hub pandas numpy tqdm requests python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary modules\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import our custom modules\n",
        "from providers import ProviderManager\n",
        "from reasoning_approaches_corrected import ReasoningApproachManager  # Use corrected version\n",
        "from dataset_loader import DatasetLoader\n",
        "from experiment_runner import ExperimentRunner\n",
        "\n",
        "print(\"‚úÖ All modules imported successfully!\")\n",
        "print(\"üìö Using corrected reasoning approaches with proper citations and algorithms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API Keys Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up API keys\n",
        "# You can either set them as environment variables or input them directly\n",
        "\n",
        "# Option 1: Set environment variables (recommended for security)\n",
        "# os.environ['OPENROUTER_API_KEY'] = 'your_openrouter_key_here'\n",
        "# os.environ['ANTHROPIC_API_KEY'] = 'your_anthropic_key_here'\n",
        "# os.environ['COHERE_API_KEY'] = 'your_cohere_key_here'\n",
        "# os.environ['HUGGINGFACE_TOKEN'] = 'your_hf_token_here'\n",
        "\n",
        "# Option 2: Input directly (less secure, but convenient for testing)\n",
        "OPENROUTER_API_KEY = \"\"  # Your OpenRouter API key\n",
        "ANTHROPIC_API_KEY = \"\"    # Your Anthropic API key\n",
        "COHERE_API_KEY = \"\"       # Your Cohere API key\n",
        "HUGGINGFACE_TOKEN = \"\"    # Your HuggingFace token\n",
        "\n",
        "print(\"üîë API keys configured!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize all components\n",
        "provider_manager = ProviderManager()\n",
        "reasoning_manager = ReasoningApproachManager()\n",
        "dataset_loader = DatasetLoader(hf_token=HUGGINGFACE_TOKEN)\n",
        "experiment_runner = ExperimentRunner(provider_manager, reasoning_manager, dataset_loader)\n",
        "\n",
        "# Add providers\n",
        "if OPENROUTER_API_KEY:\n",
        "    provider_manager.add_provider(\"openrouter\", OPENROUTER_API_KEY)\n",
        "    print(\"‚úÖ OpenRouter provider added\")\n",
        "\n",
        "if ANTHROPIC_API_KEY:\n",
        "    provider_manager.add_provider(\"anthropic\", ANTHROPIC_API_KEY)\n",
        "    print(\"‚úÖ Anthropic provider added\")\n",
        "\n",
        "if COHERE_API_KEY:\n",
        "    provider_manager.add_provider(\"cohere\", COHERE_API_KEY)\n",
        "    print(\"‚úÖ Cohere provider added\")\n",
        "\n",
        "print(f\"\\nüß† Available reasoning approaches: {len(reasoning_manager.get_available_approaches())}\")\n",
        "print(f\"üè¢ Available providers: {list(provider_manager.providers.keys())}\")\n",
        "\n",
        "# Show citations for research transparency\n",
        "print(\"\\nüìö Research Citations:\")\n",
        "citations = reasoning_manager.get_citations()\n",
        "for approach, citation in citations.items():\n",
        "    print(f\"\\n{approach}:\")\n",
        "    print(f\"  {citation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset - choose one option:\n",
        "\n",
        "# Option 1: Load from HuggingFace\n",
        "dataset_name = \"gsm8k\"  # Change to your preferred dataset\n",
        "split = \"test\"\n",
        "num_samples = 5  # Limit for testing, set to None for full dataset\n",
        "\n",
        "print(f\"üì• Loading {dataset_name} dataset...\")\n",
        "df = dataset_loader.load_huggingface_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    split=split,\n",
        "    num_samples=num_samples\n",
        ")\n",
        "\n",
        "# Preprocess for our format\n",
        "df = dataset_loader.preprocess_dataset(df, input_field=\"question\", expected_field=\"answer\")\n",
        "\n",
        "# Option 2: Use sample dataset (uncomment to use)\n",
        "# df = dataset_loader.create_sample_dataset()\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded: {len(df)} samples\")\n",
        "print(f\"üìù Sample input: {df['input'].iloc[0][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Configure and Run Experiments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure experiments\n",
        "selected_approaches = [\n",
        "    \"None\",\n",
        "    \"Chain-of-Thought (CoT)\",\n",
        "    \"Least-to-Most Prompting (LtM)\",\n",
        "    \"Reasoning-as-Planning (RAP)\"\n",
        "]\n",
        "\n",
        "selected_models = [\n",
        "    \"gpt-4o-mini\",  # OpenRouter\n",
        "    \"claude-3-5-sonnet-20241022\",  # Anthropic\n",
        "    \"command-r-plus\"  # Cohere\n",
        "]\n",
        "\n",
        "max_samples = 3  # Limit for testing\n",
        "output_dir = \"experiment_results\"\n",
        "\n",
        "print(f\"üéØ Running {len(selected_approaches)} approaches √ó {len(selected_models)} models √ó {min(max_samples, len(df))} samples\")\n",
        "print(f\"üìä Total experiments: {len(selected_approaches) * len(selected_models) * min(max_samples, len(df))}\")\n",
        "\n",
        "# Run experiments\n",
        "results_df = experiment_runner.run_batch_experiments(\n",
        "    dataset=df,\n",
        "    reasoning_approaches=selected_approaches,\n",
        "    models=selected_models,\n",
        "    max_samples=max_samples,\n",
        "    output_dir=output_dir\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Experiments completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Analyze Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze results\n",
        "summary = experiment_runner.get_experiment_summary()\n",
        "print(\"üìà Experiment Summary:\")\n",
        "for key, value in summary.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "# Compare approaches\n",
        "print(\"\\nüß† Approach Comparison:\")\n",
        "approach_comparison = experiment_runner.compare_approaches()\n",
        "print(approach_comparison)\n",
        "\n",
        "# View sample results\n",
        "print(\"\\nüìã Sample Results:\")\n",
        "for i, (_, row) in enumerate(results_df.head(2).iterrows()):\n",
        "    print(f\"\\nExperiment {i+1}:\")\n",
        "    print(f\"Input: {row['input'][:80]}...\")\n",
        "    print(f\"Approach: {row['reasoning_approach']}\")\n",
        "    print(f\"Model: {row['model_name']}\")\n",
        "    print(f\"Time: {row['execution_time_s']:.2f}s\")\n",
        "    print(f\"Output: {row['model_output'][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Export Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export results\n",
        "from google.colab import files\n",
        "\n",
        "# Save and download CSV\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "filename = f\"reasoning_experiments_{timestamp}.csv\"\n",
        "results_df.to_csv(filename, index=False)\n",
        "files.download(filename)\n",
        "\n",
        "print(f\"üì• Downloaded: {filename}\")\n",
        "\n",
        "# Optional: Push to HuggingFace Hub\n",
        "push_to_hf = False  # Set to True to enable\n",
        "if push_to_hf and HUGGINGFACE_TOKEN:\n",
        "    repo_name = \"your-username/reasoning-experiments\"  # Replace with your repo\n",
        "    success = experiment_runner.push_to_huggingface(\n",
        "        df=results_df,\n",
        "        repo_name=repo_name,\n",
        "        hf_token=HUGGINGFACE_TOKEN\n",
        "    )\n",
        "    print(f\"üöÄ HuggingFace upload: {'‚úÖ Success' if success else '‚ùå Failed'}\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è  HuggingFace upload skipped\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
